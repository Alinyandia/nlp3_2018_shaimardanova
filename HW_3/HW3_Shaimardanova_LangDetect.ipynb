{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение языка (language detection)\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скачаем тексты из википедии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Выберем 10 языков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages = ['bg', 'be', 'kk', 'uk', 'fr', 'ru', 'en', 'mk', 'la', 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10): # функция для скачивания статей из википедии\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Тексас (пояснение)\n",
      "Skipping page Евксиноград (пояснение)\n",
      "Skipping page Арат (пояснение)\n",
      "Skipping page Лапландия (пояснение)\n",
      "Skipping page Келвин (пояснение)\n",
      "Skipping page Алесандро Фарнезе\n",
      "Skipping page Приют\n",
      "Skipping page Британик (пояснение)\n",
      "bg 92\n",
      "Skipping page Міхаіл Загоскін (значэнні)\n",
      "Skipping page Сташэўскі\n",
      "Skipping page Мужанка (значэнні)\n",
      "Skipping page Грамыкі\n",
      "Skipping page Знание\n",
      "be 95\n",
      "kk 100\n",
      "Skipping page Новоозерне\n",
      "Skipping page Єнішовіце\n",
      "Skipping page Абінгтон\n",
      "Skipping page Петрик\n",
      "uk 96\n",
      "Skipping page Saint-Martin-Lars-en-Sainte-Hermine\n",
      "Skipping page Lamu\n",
      "Skipping page L'Invention de Morel\n",
      "Skipping page Chabbie\n",
      "fr 96\n",
      "Skipping page Миркович\n",
      "Skipping page Резерват\n",
      "Skipping page Дурсун\n",
      "Skipping page Уикс, Джон\n",
      "Skipping page История версий Java\n",
      "Skipping page Суперлига Б 2001/2002\n",
      "Skipping page Демидовский район (Смоленская область)\n",
      "Skipping page Лебедянка\n",
      "Skipping page Вьюшков\n",
      "Skipping page День работников государственной статистики (Белоруссия)\n",
      "Skipping page Ананченко\n",
      "ru 89\n",
      "Skipping page Seh Daran\n",
      "Skipping page Liar Liar (disambiguation)\n",
      "Skipping page Four-thousander\n",
      "Skipping page Moxico\n",
      "en 96\n",
      "Skipping page Трка околу Норвешка\n",
      "Skipping page Бен Кинг\n",
      "Skipping page Штајнбург (Саксонија-Анхалт)\n",
      "mk 97\n",
      "Skipping page Leucas (discretiva)\n",
      "Skipping page Gould (discretiva)\n",
      "Skipping page Luna (discretiva)\n",
      "la 97\n",
      "Skipping page Endellion (Begriffsklärung)\n",
      "Skipping page Miklavž\n",
      "Skipping page Archipowka\n",
      "Skipping page Jean Durand\n",
      "Skipping page Authorisierung\n",
      "Skipping page Denson\n",
      "Skipping page Sand Lake\n",
      "de 93\n"
     ]
    }
   ],
   "source": [
    "wiki_texts = {}\n",
    "for lang in languages:\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import collections\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Дополнительно очистим текст от знаков перпинания, лишних пробелов/табуляцй/переносов, от цифр и тд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('(http://|https://|www.)[a-zA-z0-9]*? ', '', text)\n",
    "    text = re.sub('[^\\w\\s]', '',text)\n",
    "    text = re.sub('[\\s]{2,}', ' ',text)\n",
    "    text = re.sub('[0-9]+', '',text)\n",
    "    return text.split(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первый метод: частотные слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем частотный словарь (кортеж словарей, получается) для слов в языке.\n",
    "\n",
    "Ключ -- это язык, а значение -- это словарь, где ключ -- это слово, а значение -- это частотность слова во всем словаре (корпусе) для одного языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dict = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала просто частотный словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    for article in wiki_texts[lang]:\n",
    "        for word in tokenize(article.replace('\\n', '').lower()):\n",
    "            freq_dict[lang][word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потом меняем частоту на абсолютное значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    for word in freq_dict[lang]:\n",
    "        freq_dict[lang][word] = freq_dict[lang][word]/sum(freq_dict[lang].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что у нас там:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24532825706742278\tжалғастырғышты\n",
      "0.12463899494783258\tжүріс\n",
      "0.11222148334254944\tтежегішті\n",
      "0.10194716438342984\tжөне\n",
      "0.09448440692641817\tтісті\n",
      "0.09332117678404195\tтүзілімді\n",
      "0.08598678049100969\tүйкеліс\n",
      "0.07968118951576447\tүйлестіргіштері\n",
      "0.07263155251366435\tберілістер\n",
      "0.059803047545895646\tқорабы\n"
     ]
    }
   ],
   "source": [
    "for word in sorted(freq_dict['kk'], key=lambda w: freq_dict['kk'][w], reverse=True)[:10]:\n",
    "    print('{}\\t{}'.format(freq_dict['kk'][word], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language(text, freq_dict):\n",
    "    words_var = {}\n",
    "    for lang in freq_dict.keys():\n",
    "        count = 0\n",
    "        for word in tokenize(text):\n",
    "            if word in freq_dict[lang]:\n",
    "                count += freq_dict[lang][word]\n",
    "        words_var[lang] = count\n",
    "    \n",
    "    result = max(words_var, key=lambda x: words_var[x]) \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('ich liebe dich', freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, всё намного сложнее с языками, где присутствует кириллица:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ru'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('Привет. Как дела? Какой это язык, скажешь?', freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mk'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('Что это за язык, скажешь?', freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uk'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('Привет. определи язык, пожалуйста!', freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не очень поняла, что подразумевается под \"удалить повторения из словарей\":(\n",
    "\n",
    "Самым логичным трактованием этого задания мне показалось: удалить одинаковые единицы из словарей, то есть сделать разность словарей. В число удалений, скорей всего, войдут служебные слова у языков одной языковой семьи, а также возможные деривации, которые не деформировались в результате заимствования. Однако, мы можем и не удалять эти повторения из словарей, потому что они никак не должны повлиять на результаты определения языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in languages:\n",
    "    for text in wiki_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(predict_language(text, freq_dict))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       1.00      0.91      0.95        95\n",
      "         bg       0.69      0.98      0.81        92\n",
      "         de       0.99      0.98      0.98        93\n",
      "         en       0.91      0.99      0.95        96\n",
      "         fr       0.95      0.99      0.97        96\n",
      "         kk       0.99      1.00      1.00       100\n",
      "         la       1.00      0.85      0.92        97\n",
      "         mk       0.96      0.95      0.95        97\n",
      "         ru       0.83      0.71      0.76        89\n",
      "         uk       0.99      0.85      0.92        96\n",
      "\n",
      "avg / total       0.93      0.92      0.92       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 86   3   0   0   0   0   0   2   3   1]\n",
      " [  0  90   0   1   0   0   0   1   0   0]\n",
      " [  0   0  91   0   0   0   0   0   2   0]\n",
      " [  0   0   0  95   0   1   0   0   0   0]\n",
      " [  0   0   0   0  95   0   0   0   1   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0   1   6   5   0  82   0   3   0]\n",
      " [  0   4   0   0   0   0   0  92   1   0]\n",
      " [  0  26   0   0   0   0   0   0  63   0]\n",
      " [  0   8   0   2   0   0   0   1   3  82]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй метод: частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, которая преобразовывает строку в массив n-грамм заданной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('(http://|https://|www.)[a-zA-z0-9]*? ', '', text)\n",
    "    text = re.sub('[^\\w\\s]', '',text)\n",
    "    text = re.sub('[\\s]{2,}', ' ',text)\n",
    "    text = re.sub('[0-9]+', '',text)\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ich',\n",
       " 'ch ',\n",
       " 'h h',\n",
       " ' he',\n",
       " 'hei',\n",
       " 'eiß',\n",
       " 'iße',\n",
       " 'ße ',\n",
       " 'e a',\n",
       " ' al',\n",
       " 'ali',\n",
       " 'lin',\n",
       " 'ina']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_ngrams('ich heiße Alina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим частотные словари n-грамм аналогично первому методу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_ndrm = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    for article in wiki_texts[lang]:\n",
    "        for word in make_ngrams(article.replace('\\n', '').lower()):\n",
    "            freq_ndrm[lang][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    for word in freq_ndrm[lang]:\n",
    "        freq_ndrm[lang][word] = freq_ndrm[lang][word]/sum(freq_ndrm[lang].values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что у нас там."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12213749127777604\tжиі\n",
      "0.10104254400129953\tжег\n",
      "0.09255603943632926\tтеж\n",
      "0.08533199900519951\tүйл\n",
      "0.07911506206267573\tжұд\n",
      "0.07371312515600513\tону\n",
      "0.06897919002189508\tжон\n",
      "0.05774591435037658\tүра\n",
      "0.05475705951060114\tзм \n",
      "0.05205505456591226\tпқо\n"
     ]
    }
   ],
   "source": [
    "for word in sorted(freq_ndrm['kk'], key=lambda w: freq_ndrm['kk'][w], reverse=True)[:10]:\n",
    "    print('{}\\t{}'.format(freq_ndrm['kk'][word], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_predict_language(text, freq_ndrm):\n",
    "    ngram_var = {}\n",
    "    \n",
    "    for lang in freq_ndrm.keys():\n",
    "        count = 0\n",
    "        for word in make_ngrams(text):\n",
    "            if word in freq_ndrm[lang]:\n",
    "                count += freq_ndrm[lang][word]\n",
    "        ngram_var[lang] = count\n",
    "    \n",
    "    return max(ngram_var, key=lambda x: ngram_var[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_predict_language(text, freq_ndrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mk'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_predict_language('Привет. Как дела? Какой это язык, скажешь?', freq_ndrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mk'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_predict_language('Что это за язык, скажешь?', freq_ndrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kk'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_predict_language('Привет. определи язык, пожалуйста!', freq_ndrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in languages:\n",
    "    for text in wiki_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(ngram_predict_language(text, freq_ndrm))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       1.00      0.79      0.88        95\n",
      "         bg       0.65      0.33      0.43        92\n",
      "         de       0.95      0.99      0.97        93\n",
      "         en       0.97      0.97      0.97        96\n",
      "         fr       0.98      1.00      0.99        96\n",
      "         kk       0.95      0.99      0.97       100\n",
      "         la       0.97      0.97      0.97        97\n",
      "         mk       0.41      0.99      0.58        97\n",
      "         ru       0.96      0.28      0.43        89\n",
      "         uk       0.82      0.66      0.73        96\n",
      "\n",
      "avg / total       0.87      0.80      0.80       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[75  1  1  0  0  1  0  5  0 12]\n",
      " [ 0 30  0  1  0  0  0 61  0  0]\n",
      " [ 0  0 92  0  0  0  1  0  0  0]\n",
      " [ 0  0  0 93  2  0  0  1  0  0]\n",
      " [ 0  0  0  0 96  0  0  0  0  0]\n",
      " [ 0  1  0  0  0 99  0  0  0  0]\n",
      " [ 0  0  3  0  0  0 94  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 96  1  0]\n",
      " [ 0  7  0  0  0  2  2 51 25  2]\n",
      " [ 0  7  1  2  0  2  0 21  0 63]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
